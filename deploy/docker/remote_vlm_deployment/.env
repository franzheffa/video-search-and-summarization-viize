export NVIDIA_API_KEY=nvapi-*** #FIXME - api key to access NIM endpoints. Should come from build.nvidia.com
export OPENAI_API_KEY=sk-*** #FIXME - api key for the remote VLM 
export NGC_API_KEY=abc123*** #api key to pull model from NGC. Add this if you are not using the jupyter notebooks

#Use latest public image 2.3.0.
export VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.3.0

#Adjust ports if needed
export FRONTEND_PORT=9100
export BACKEND_PORT=8100

#Change default user and pass if needed 
export GRAPH_DB_USERNAME=neo4j
export GRAPH_DB_PASSWORD=password

#Update paths local paths to config files if needed. If it appears VSS is not using these configurations, then change the relative paths to absolute paths. 
export CA_RAG_CONFIG=./config.yaml
export GUARDRAILS_CONFIG=./guardrails

export ENABLE_AUDIO=false

export DISABLE_CV_PIPELINE=true

export RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
export RIVA_ASR_GRPC_PORT=443
export RIVA_ASR_SERVER_IS_NIM=true
export RIVA_ASR_SERVER_USE_SSL=true
export RIVA_ASR_SERVER_API_KEY=nvapi-***
export RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"

#Set VLM to OpenAI model 
export VLM_MODEL_TO_USE=openai-compat
export VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME=gpt-4o #FIX ME - change VLM model on remote endpoint
export VIA_VLM_ENDPOINT=https://api.openai.com/v1/chat/completions #FIX ME - change url to point to remote VLM endpoint. Can be any VLM with an openAI compatible API.

#Adjust misc configs if needed
export DISABLE_GUARDRAILS=false
export NVIDIA_VISIBLE_DEVICES=0 #If you system has more than 1 GPU, configure which GPUs to use to run VSS. VSS can utilize multiple GPUs for lower latency summarization.
